# -*- coding: utf-8 -*-
"""189HW5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nayXrDGter_OVcE_HJss-mLd3aqDfG_Z
"""

from google.colab import drive
drive.mount('/content/gdrive')

!unzip -q "/content/gdrive/My Drive/Sophomore/hw5_code.zip"

ls

cd hw5_code

ls

# You may want to install "gprof2dot"
import io
from collections import Counter

import numpy as np
import scipy.io
import sklearn.model_selection
import sklearn.tree
from numpy import genfromtxt
from scipy import stats
from sklearn.base import BaseEstimator, ClassifierMixin
import matplotlib.pyplot as plt

import pydot

!pip install -q pydot

np.random.seed(42)

np.random.seed(42)
eps = 1e-5  # a small number


class DecisionTree:
    def __init__(self, max_depth=3, feature_labels=None, m = None):
        self.max_depth = max_depth
        self.features = feature_labels
        self.left, self.right = None, None  # for non-leaf nodes
        self.split_idx, self.thresh = None, None  # for non-leaf nodes
        self.data, self.pred = None, None  # for leaf nodes
        #add variables
        self.m = m

    def entropy(labels):
      classes, counts = np.unique(labels, return_counts = True)
      total = np.sum(counts)
      entro = 0
      for i in range(len(classes)):
        p_c = counts[i] / total
        entro = entro + ((-1) * p_c * np.log2(p_c))
      return entro

    @staticmethod
    def information_gain(X, y, thresh):
        # TODO: implement information gain function
        H_S = DecisionTree.entropy(y)
        S_left = y[np.where(X < thresh)]
        S_right = y[np.where(X >= thresh)]
        entro_left = DecisionTree.entropy(S_left)
        entro_right = DecisionTree.entropy(S_right)
        H_after = (len(S_left) * entro_left + len(S_right) * entro_right) / (len(S_left) + len(S_right))
        return H_S - H_after

    @staticmethod
    def gini_impurity(X, y, thresh):
        # TODO: implement gini impurity function
        pass

    def split(self, X, y, idx, thresh):
        X0, idx0, X1, idx1 = self.split_test(X, idx=idx, thresh=thresh)
        y0, y1 = y[idx0], y[idx1]
        return X0, y0, X1, y1

    def split_test(self, X, idx, thresh):
        idx0 = np.where(X[:, idx] < thresh)[0]
        idx1 = np.where(X[:, idx] >= thresh)[0]
        X0, X1 = X[idx0, :], X[idx1, :]
        return X0, idx0, X1, idx1

    def fit(self, X, y):
        if self.max_depth > 0:
            # compute entropy gain for all single-dimension splits,
            # thresholding with a linear interpolation of 10 values
            gains = []
            # The following logic prevents thresholding on exactly the minimum
            # or maximum values, which may not lead to any meaningful node
            # splits.
            full_data = X
            if self.m:
              # print("self.features:", self.features)
              selected_features = np.random.choice(list(range(len(self.features))),
                                                   size=self.m, replace=False)
              # print("selected features:", selected_features)
              X = full_data[:, selected_features]
            # print("X shape:", X.shape)
            thresh = np.array([
                np.linspace(np.min(X[:, i]) + eps, np.max(X[:, i]) - eps, num=10)
                for i in range(X.shape[1])
            ])
            for i in range(X.shape[1]):
                gains.append([self.information_gain(X[:, i], y, t) for t in thresh[i, :]])

            gains = np.nan_to_num(np.array(gains))
            self.split_idx, thresh_idx = np.unravel_index(np.argmax(gains), gains.shape)
            # print("split_idx:", self.split_idx)
            self.thresh = thresh[self.split_idx, thresh_idx]

            if self.m:
              # print("in self.m for split idx")
              self.split_idx = selected_features[self.split_idx]
              # print("split_idx now is:", self.split_idx)

            X0, y0, X1, y1 = self.split(full_data, y, idx=self.split_idx, thresh=self.thresh)
            if X0.size > 0 and X1.size > 0:
                self.left = DecisionTree(
                    max_depth=self.max_depth - 1, feature_labels=self.features, m = self.m)
                self.left.fit(X0, y0)
                self.right = DecisionTree(
                    max_depth=self.max_depth - 1, feature_labels=self.features, m = self.m)
                self.right.fit(X1, y1)
            else:
                self.max_depth = 0
                self.data, self.labels = full_data, y #modified
                self.pred = stats.mode(y).mode[0]
        else:
            self.data, self.labels = X, y
            self.pred = stats.mode(y).mode[0]
        return self

    def predict(self, X):
        if self.max_depth == 0:
            return self.pred * np.ones(X.shape[0])
        else:
            X0, idx0, X1, idx1 = self.split_test(X, idx=self.split_idx, thresh=self.thresh)
            yhat = np.zeros(X.shape[0])
            yhat[idx0] = self.left.predict(X0)
            yhat[idx1] = self.right.predict(X1)
            return yhat

    def __repr__(self):
        if self.max_depth == 0:
            return "%s (%s)" % (self.pred, self.labels.size)
        else:
            return "[%s < %s: %s | %s]" % (self.features[self.split_idx],
                                           self.thresh, self.left.__repr__(),
                                           self.right.__repr__())

    def is_leaf(self):
        return self.left is None and self.right is None

    def __str__(self):
        def print_tree(t, indent):
            if t.is_leaf():
              return '        ' * indent + str(titanic_binary_convert(t.pred)) + "\n"
            tree_str = '        ' * indent + str(titanic_convert(t.split_idx)) + " < " + str(t.thresh) + "\n"
            if t.left is not None:
                tree_str += print_tree(t.left, indent + 1)
            if t.right is not None:
                tree_str += print_tree(t.right, indent + 1)
            return tree_str
        return print_tree(self, 0).rstrip()


def titanic_binary_convert(idx):
    if idx == 1:
        return "Survived"
    else:
        return "Died"

def titanic_convert(idx):
    f = [
            "pclass", "sex", "age", "sibsp", "parch", "ticket", "fare", "cabin", "embarked",
            "male", "female", "S", "C", "Q"
        ]
    return f[idx]


class BaggedTrees(BaseEstimator, ClassifierMixin):
    def __init__(self, params=None, n=20): # tune n
        if params is None:
            params = {}
        self.params = params
        self.n = n
        self.decision_trees = [
            DecisionTree(**params)
            for i in range(self.n)
        ]

    def fit(self, X, y):
        # TODO: implement function
        for t in self.decision_trees:
            # Bagging: randomly sample with replacement from X and y
            idx = np.random.choice(X.shape[0], size=X.shape[0], replace=True)
            X_sampled, y_sampled = X[idx], y[idx]

            # Train decision tree on sampled data
            t.fit(X_sampled, y_sampled)

    def predict(self, X):
        # TODO: implement function
        predictions = []
        for tree in self.decision_trees:
            predictions.append(tree.predict(X))

        # Ensemble: majority vote
        final_preds = np.vstack(predictions)
        most_common = stats.mode(final_preds)[0][0]
        # print("most_common:", most_common)
        return most_common


class RandomForest(BaggedTrees):
    def __init__(self, params=None, n=20, m=6): #Tune m
        if params is None:
            params = {}
        params["m"] = m
        # print("params:", params)
        super().__init__(params=params, n=n)


class BoostedRandomForest(RandomForest):
    def fit(self, X, y):
        self.w = np.ones(X.shape[0]) / X.shape[0]  # Weights on data
        self.a = np.zeros(self.n)  # Weights on decision trees
        # TODO: implement function
        return self

    def predict(self, X):
        # TODO: implement function
        pass


def preprocess(data, fill_mode=True, min_freq=10, onehot_cols=[]):
    # fill_mode = False

    # Temporarily assign -1 to missing data
    data[data == ''] = '-1'

    # Hash the columns (used for handling strings)
    onehot_encoding = []
    onehot_features = []
    for col in onehot_cols:
        counter = Counter(data[:, col])
        for term in counter.most_common():
            if term[0] == '-1':
                continue
            if term[-1] <= min_freq:
                break
            onehot_features.append(term[0])
            onehot_encoding.append((data[:, col] == term[0]).astype(float))
        data[:, col] = '0'
    onehot_encoding = np.array(onehot_encoding).T
    data = np.hstack([np.array(data, dtype=float), np.array(onehot_encoding)])

    # Replace missing data with the mode value. We use the mode instead of
    # the mean or median because this makes more sense for categorical
    # features such as gender or cabin type, which are not ordered.
    if fill_mode:
        for i in range(data.shape[-1]):
            mode = stats.mode(data[((data[:, i] < -1 - eps) +
                                    (data[:, i] > -1 + eps))][:, i]).mode[0]
            data[(data[:, i] > -1 - eps) * (data[:, i] < -1 + eps)][:, i] = mode

    return data, onehot_features


def evaluate(clf):
    print("Cross validation", sklearn.model_selection.cross_val_score(clf, X, y))
    if hasattr(clf, "decision_trees"):
        counter = Counter([t.tree_.feature[0] for t in clf.decision_trees])
        first_splits = [(features[term[0]], term[1]) for term in counter.most_common()]
        print("First splits", first_splits)


# if __name__ == "__main__":
#     dataset = "titanic"
#     params = {
#         "max_depth": 5,
#         # "random_state": 6,
#         "min_samples_leaf": 10,
#     }
#     N = 100

#     if dataset == "titanic":
#         # Load titanic data
#         path_train = './dataset/titanic/titanic_training.csv'
#         data = genfromtxt(path_train, delimiter=',', dtype=None, encoding=None)
#         path_test = './dataset/titanic/titanic_test_data.csv'
#         test_data = genfromtxt(path_test, delimiter=',', dtype=None, encoding=None)
#         y = data[1:, -1]  # label = survived
#         class_names = ["Died", "Survived"]
#         labeled_idx = np.where(y != '')[0]

#         y = np.array(y[labeled_idx])
#         y = y.astype(float).astype(int)


#         print("\n\nPart (b): preprocessing the titanic dataset")
#         X, onehot_features = preprocess(data[1:, :-1], onehot_cols=[1, 5, 7, 8])
#         X = X[labeled_idx, :]
#         Z, _ = preprocess(test_data[1:, :], onehot_cols=[1, 5, 7, 8])
#         assert X.shape[1] == Z.shape[1]
#         features = list(data[0, :-1]) + onehot_features

#     elif dataset == "spam":
#         features = [
#             "pain", "private", "bank", "money", "drug", "spam", "prescription", "creative",
#             "height", "featured", "differ", "width", "other", "energy", "business", "message",
#             "volumes", "revision", "path", "meter", "memo", "planning", "pleased", "record", "out",
#             "semicolon", "dollar", "sharp", "exclamation", "parenthesis", "square_bracket",
#             "ampersand"
#         ]
#         assert len(features) == 32

#         # Load spam data
#         path_train = './dataset/spam/spam_data.mat'
#         data = scipy.io.loadmat(path_train)
#         X = data['training_data']
#         y = np.squeeze(data['training_labels'])
#         Z = data['test_data']
#         class_names = ["Ham", "Spam"]

#     else:
#         raise NotImplementedError("Dataset %s not handled" % dataset)

#     print("Features:", features)
#     print("Train/test size:", X.shape, Z.shape)

#     print("\n\nPart 0: constant classifier")
#     print("Accuracy", 1 - np.sum(y) / y.size)

#     # Basic decision tree
#     print("\n\nPart (a-b): simplified decision tree")
#     dt = DecisionTree(max_depth=3, feature_labels=features)
#     dt.fit(X, y)
#     print("Predictions", dt.predict(Z)[:100])

#     print("\n\nPart (c): sklearn's decision tree")
#     clf = sklearn.tree.DecisionTreeClassifier(random_state=0, **params)
#     clf.fit(X, y)
#     evaluate(clf)
#     out = io.StringIO()

#     # You may want to install "gprof2dot"
#     sklearn.tree.export_graphviz(
#         clf, out_file=out, feature_names=features, class_names=class_names)
#     graph = pydot.graph_from_dot_data(out.getvalue())
#     pydot.graph_from_dot_data(out.getvalue())[0].write_pdf("%s-tree.pdf" % dataset)

    # TODO: implement and evaluate!

#validation time
path_train = './dataset/spam/spam_data.mat'
spam = scipy.io.loadmat(path_train)
spam_features = spam['training_data']
spam_labels = np.squeeze(spam['training_labels'])

np.random.seed(42)
# Train/Validation Split Function
def shuffle_and_partition(data, labels, val_size):
  data_size = len(data)
  if val_size < 1.0:
    val_size = int(data_size * val_size)
  training_size = data_size - val_size
  np.random.seed(42)
  index_perm = np.random.permutation(data_size)
  train_index = index_perm[:training_size]
  val_index= index_perm[training_size:]
  train_pts = data[train_index]
  train_labels = labels[train_index]
  val_pts = data[val_index]
  val_labels = labels[val_index]

  return train_pts, train_labels, val_pts, val_labels

# Spam Training/Validation Split
spam_train_features, spam_train_labels, spam_val_features, spam_val_labels = shuffle_and_partition(
    spam_features, spam_labels, 0.20)

def accuracy_rate(predicted_labels, actual_labels):
  return np.sum(predicted_labels == actual_labels) / len(actual_labels)

# 4.4 Performance Evaluation Spam Decision Tree
features = [
            "pain", "private", "bank", "money", "drug", "spam", "prescription", "creative",
            "height", "featured", "differ", "width", "other", "energy", "business", "message",
            "volumes", "revision", "path", "meter", "memo", "planning", "pleased", "record", "out",
            "semicolon", "dollar", "sharp", "exclamation", "parenthesis", "square_bracket",
            "ampersand"
        ]
simple_dt = DecisionTree(max_depth=10, feature_labels=features)
simple_dt.fit(spam_train_features, spam_train_labels)
val_preds = simple_dt.predict(spam_val_features)

# Spam Decision Tree Training Accuracy
spam_dt_train_pred = simple_dt.predict(spam_train_features)
accuracy_rate(spam_dt_train_pred, spam_train_labels)

# Spam Decision Tree Validation Accuracy
accuracy_rate(val_preds, spam_val_labels)

# Performance Evaluation Spam RandomForest
np.random.seed(42)
features = [
            "pain", "private", "bank", "money", "drug", "spam", "prescription", "creative",
            "height", "featured", "differ", "width", "other", "energy", "business", "message",
            "volumes", "revision", "path", "meter", "memo", "planning", "pleased", "record", "out",
            "semicolon", "dollar", "sharp", "exclamation", "parenthesis", "square_bracket",
            "ampersand"
        ]
params = {"max_depth": 10, "feature_labels": features}
rf = RandomForest(params, n=20, m = 6)
rf.fit(spam_train_features, spam_train_labels)
rf_preds = rf.predict(spam_val_features)

# Spam Random Forest Training Accuracy
rf_train_preds = rf.predict(spam_train_features)
accuracy_rate(rf_train_preds, spam_train_labels)

# Spam Random Forest Validation Accuracy
accuracy_rate(rf_preds, spam_val_labels)

# Performance Evaluation Titanic Decision Tree
# preprocess titanic data
path_train = './dataset/titanic/titanic_training.csv'
data = genfromtxt(path_train, delimiter=',', dtype=None, encoding=None)
path_test = './dataset/titanic/titanic_test_data.csv'
test_data = genfromtxt(path_test, delimiter=',', dtype=None, encoding=None)
y = data[1:, -1]  # label = survived
class_names = ["Died", "Survived"]
labeled_idx = np.where(y != '')[0]
y = np.array(y[labeled_idx])
y = y.astype(float).astype(int)
X, onehot_features = preprocess(data[1:, :-1], onehot_cols=[1, 5, 7, 8])
X = X[labeled_idx, :]
Z, _ = preprocess(test_data[1:, :], onehot_cols=[1, 5, 7, 8])

features = list(data[0, :-1]) + onehot_features

# Train/Validation Split for Titanic
titanic_train_features, titanic_train_labels, titanic_val_features, titanic_val_labels = shuffle_and_partition(
    X, y, 0.20)

# Performance Evaluation on decision tree for titanic
titanic_features = [
            "pclass", "sex", "age", "sibsp", "parch", "ticket", "fare", "cabin", "embarked",
            "male", "female", "S", "C", "Q"
        ]
titanic_dt = DecisionTree(max_depth=5, feature_labels=titanic_features)
titanic_dt.fit(titanic_train_features, titanic_train_labels)
titanic_val_preds = titanic_dt.predict(titanic_val_features)

# Titanic Decision Tree Training Accuracy
titanic_train_preds = titanic_dt.predict(titanic_train_features)
accuracy_rate(titanic_train_preds, titanic_train_labels)

# Titanic Decision Tree Validation Accuracy
accuracy_rate(titanic_val_preds, titanic_val_labels)

np.random.seed(42)
# Performance Evaluation Titanic RandomForest
titanic_params = {"max_depth": 6, "feature_labels": titanic_features}
titanic_rf = RandomForest(titanic_params, n=20, m=4)
titanic_rf.fit(titanic_train_features, titanic_train_labels)
titanic_rf_preds = titanic_rf.predict(titanic_val_features)

# Titanic Random Forest Training Accuracy
titanic_train_preds = titanic_rf.predict(titanic_train_features)
accuracy_rate(titanic_train_preds, titanic_train_labels)

# Titanic Random Forest Validation Accuracy
accuracy_rate(titanic_rf_preds, titanic_val_labels)

# Begin to Train Best Model For Both Datasets
import pandas as pd
def results_to_csv(y_test):
    y_test = y_test.astype(int)
    df = pd.DataFrame({'Category': y_test})
    df.index += 1 # Ensures that the index starts at 1
    df.to_csv('submission.csv', index_label='Id')

# Spam Best Model
# train BaggedTrees model for spam test data
features = [
            "pain", "private", "bank", "money", "drug", "spam", "prescription", "creative",
            "height", "featured", "differ", "width", "other", "energy", "business", "message",
            "volumes", "revision", "path", "meter", "memo", "planning", "pleased", "record", "out",
            "semicolon", "dollar", "sharp", "exclamation", "parenthesis", "square_bracket",
            "ampersand"
        ]
params = {"max_depth": 10, "feature_labels": features}
bt = BaggedTrees(params, n=50)
bt.fit(spam_features, spam_labels)
bt_preds_submission = bt.predict(spam["test_data"])

# Convert predictions to csv for Spam submission
results_to_csv(bt_preds_submission)

# Titanic Best Model
# train BaggedTree Model for titanic
titanic_features = [
            "pclass", "sex", "age", "sibsp", "parch", "ticket", "fare", "cabin", "embarked",
            "male", "female", "S", "C", "Q"
        ]
titanic_params = {"max_depth": 3, "feature_labels": titanic_features} #submission1 max_depth = 5
titanic_bt = BaggedTrees(titanic_params, n=200) #submission1 n = 50
titanic_bt.fit(X, y)
titanic_bt_preds_submission = titanic_bt.predict(Z)

# Convert predictions to csv for Titanic submission
results_to_csv(titanic_bt_preds_submission)

# Here begins 4.5

# 4.5.2 Spam Decision Tree
features = [
            "pain", "private", "bank", "money", "drug", "spam", "prescription", "creative",
            "height", "featured", "differ", "width", "other", "energy", "business", "message",
            "volumes", "revision", "path", "meter", "memo", "planning", "pleased", "record", "out",
            "semicolon", "dollar", "sharp", "exclamation", "parenthesis", "square_bracket",
            "ampersand"
        ]
simple_dt = DecisionTree(max_depth=3, feature_labels=features)
simple_dt.fit(spam_train_features, spam_train_labels)
val_preds = simple_dt.predict(spam_val_features)

# a helper method for converting number to actual feature name and printing the corresponding values
def spam_print_features(feature_list):
  features = [
            "pain", "private", "bank", "money", "drug", "spam", "prescription", "creative",
            "height", "featured", "differ", "width", "other", "energy", "business", "message",
            "volumes", "revision", "path", "meter", "memo", "planning", "pleased", "record", "out",
            "semicolon", "dollar", "sharp", "exclamation", "parenthesis", "square_bracket",
            "ampersand"
        ]
  for i in range(len(feature_list)):
    print(features[i] + ": " + str(feature_list[i]))

# visualize the tree
simple_dt

# find a spam data point
spam_train_labels[-1]

# spam data point
spam_print_features(spam_train_features[-1])

# find a ham data point
spam_train_labels[0]

# ham data point
spam_print_features(spam_train_features[0])

# 4.5.3: Graphing Validation Accuracy vs Maximum Depth
np.random.seed(42)
features = [
            "pain", "private", "bank", "money", "drug", "spam", "prescription", "creative",
            "height", "featured", "differ", "width", "other", "energy", "business", "message",
            "volumes", "revision", "path", "meter", "memo", "planning", "pleased", "record", "out",
            "semicolon", "dollar", "sharp", "exclamation", "parenthesis", "square_bracket",
            "ampersand"
        ]
accuracies = []
for i in range(1, 40):
  params = {"max_depth": i, "feature_labels": features}
  dt = DecisionTree(**params)
  dt.fit(spam_train_features, spam_train_labels)
  dt_preds = dt.predict(spam_val_features)
  accuracy = accuracy_rate(dt_preds, spam_val_labels)
  accuracies.append(accuracy)

plt.figure()
plt.plot(list(range(1,40)), accuracies)
plt.title("Decision tree Validation Accuracy using depth from 1 to 40")
plt.xlabel("Maximum Depth")
plt.ylabel("Validation Accuracy")

#4.6 visualize a 3-layer decision tree for titanic

titanic_dt = DecisionTree(max_depth=3, feature_labels=titanic_features)
titanic_dt.fit(titanic_train_features, titanic_train_labels)
print(titanic_dt)